{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Algorithm Understanding\n",
        "\n",
        "How does the Gradient-Boosted Tree Algorithm work in Classification? How does Gradient Boost differ from AdaBoost and Logistic Regression?"
      ],
      "metadata": {
        "id": "GwMFlVoLYgMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: \n",
        "Gradient-Boosted Tree Algorithm ensembles multiple weak learners(each weak learner is a decision tree) to improve the performance of a machine learning model. Combined, their output results in better models. The weak learners work sequentially. Each decision tree tries to improve on the error from the previous model. For classification, the final result is computed as the class with the majority of votes from weak learners. \n",
        "\n",
        "AdaBoost also fits a sequence of weak learners to the data. It focus on the cases where the predecessor fail by giving more weights to the mis-classified training instances.\n",
        "\n",
        "Both Gradient Boost and AdaBoost are decision tree based algorithm while logistic regression has a linear decision boundary."
      ],
      "metadata": {
        "id": "RezXG8MOZaKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Interview Readiness\n",
        "\n",
        "What is a Delta Lake and how does it offer a solution to building reliable data pipelines?"
      ],
      "metadata": {
        "id": "90_nIDjuYhCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: Delta Lake is the optimized storage layer that provides the foundation for storing data and tables in the Databricks Lakehouse Platform. \n",
        "It unifies ACID Transactions, Scalable Metadata Management, and Batch and Streaming Data Processing. "
      ],
      "metadata": {
        "id": "6UULfnBoZewN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Interview Readiness\n",
        "\n",
        "When working with Pandas, we use the class pandas.core.frame.DataFrame and when working with the pandas API in Spark, we use the class pyspark.pandas.frame.DataFrame, are these the same, explain why or why not?"
      ],
      "metadata": {
        "id": "43pLNaSNYhjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: They are different. The first one(`pandas.core.frame.DataFrame`) is Pandas DataFram, the other(`pyspark.pandas.frame.DataFrame`) is Spark DataFrame. Spark DataFrame supports parallelization while Pandas DataFrame doesn't. Spark DF is distributed while Pandas DF is not. Spark DF follows Lazy Execution while Pandas DF follows Eager Execution."
      ],
      "metadata": {
        "id": "etFQrfgxZgoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Interview Readiness\n",
        "\n",
        "What is a Machine Learning Pipeline is and why itâ€™s important? What are the steps in a Machine Learning workflow?"
      ],
      "metadata": {
        "id": "-X1Tpq0TZV-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: A machine learning pipeline is the end-to-end construct that manage the flow of data into, and output from, a machine learning model (or set of multiple models). It includes raw data input, features, outputs, the machine learning model and model parameters, and prediction outputs. \n",
        "Many real-world machine learning use cases involve complex, multi-step pipelines. Each step may require different libraries and runtimes and may need to execute on specialized hardware profiles. It is therefore critical to factor in management of libraries, runtimes, and hardware profiles during algorithm development and ongoing maintenance activities. \n",
        "\n",
        "Machine Learning workflows vary by project, but typically includs these steps:\n",
        "1. Gathering data\n",
        "2. Data pre-processing\n",
        "3. Researching the model that will be best for the type of data and task\n",
        "4. Building datasets(Train, Test, Validation)\n",
        "5. Train the model on data:\n",
        "  1. Train model(Train dataset)\n",
        "  2. Tune hyperparameters (Validation dataset)\n",
        "  3. Evaluate model accuracy(test dataset)\n",
        "6. Deploy the trained model.\n",
        "7. Manage the models and model versions."
      ],
      "metadata": {
        "id": "Q4jJ06EFZidL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UoHSCsN-X6Iy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}